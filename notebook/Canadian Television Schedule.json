{
	"name": "Canadian Television Schedule",
	"properties": {
		"folder": {
			"name": "Data Analysis with Python and PySpark"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ivsspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "65978747-c807-41d9-8a7d-2bffd24c97e1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f939f8f3-2652-4a45-88ae-78a7782ec9df/resourceGroups/ivs-dwh-rg/providers/Microsoft.Synapse/workspaces/ivs-synapse/bigDataPools/ivsspark",
				"name": "ivsspark",
				"type": "Spark",
				"endpoint": "https://ivs-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ivsspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Simple Data Frame"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"\r\n",
					"my_grocery_list = (\r\n",
					"    ['Banana', 2, 1.74],\r\n",
					"    ['Apple', 4, 2.04],\r\n",
					"    ['Carrot', 1, 1.09],\r\n",
					"    ['Cake', 1, 10.99]\r\n",
					")\r\n",
					"\r\n",
					"df_grocery_list = spark.createDataFrame(\r\n",
					"    my_grocery_list, ['Item', 'Quantity', 'Price']\r\n",
					")\r\n",
					"\r\n",
					"df_grocery_list.printSchema()"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Read the source"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"\r\n",
					"#Load text file to data frame\r\n",
					"df_tv_sched = spark.read.text(adls_path + \"/data/broadcast_logs/BroadcastLogs_2018_Q3_M8_sample.CSV\")\r\n",
					"\r\n",
					"#df_tv_sched.show(5, 500)\r\n",
					"\r\n",
					"#Parse csv to data frame\r\n",
					"logs = spark.read.csv(\r\n",
					"    path = adls_path + \"/data/broadcast_logs/BroadcastLogs_2018_Q3_M8_sample.CSV\",\r\n",
					"    sep = \"|\",\r\n",
					"    inferSchema= True,\r\n",
					"    header = True,\r\n",
					"    timestampFormat = \"yyyy-MM-dd\")\r\n",
					"\r\n",
					"logs.printSchema()\r\n",
					"\r\n",
					"#logs.show()"
				],
				"execution_count": 36
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Basic operations"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#select\r\n",
					"logs.select(\"BroadcastLogID\", \"LogServiceID\", \"LogDate\").show(5)\r\n",
					"\r\n",
					"logs.select([\"BroadCastLogID\", \"LogServiceID\", \"LogDate\"]).show(5)\r\n",
					"\r\n",
					"logs.select(F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\")).show(5)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Peeking at the data frame in chunks of three columns"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import numpy as np\r\n",
					"\r\n",
					"column_split = np.array_split(\r\n",
					"    np.array(logs.columns), len(logs.columns) // 3 #// means integer division\r\n",
					")  \r\n",
					" \r\n",
					"print(column_split)\r\n",
					"\r\n",
					"for x in column_split:\r\n",
					"    logs.select(*x).show(5) #asterix explodes array into list of values\r\n",
					"    #print(*x)\r\n",
					"    #print(x)\r\n",
					"\r\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Show data frame in table format"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(logs.take(10))"
				],
				"execution_count": 27
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Remove columns from dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(logs.drop(\"BroadCastLogID\", \"LogServiceID\").take(10))\r\n",
					"\r\n",
					"#Check if column is a part of data frame\r\n",
					"print(\"BroadCastLogID\" in logs.columns)\r\n",
					"\r\n",
					"#same with select (loop on columns array)\r\n",
					"display(logs.select([x for x in logs.columns if x not in [\"BroadcastLogID\", \"LogServiceID\"]]).take(5))"
				],
				"execution_count": 28
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 4.1\r\n",
					"Let’s take the following file, called sample.csv, which contains three columns:\r\n",
					"\r\n",
					"Item,Quantity,Price\r\n",
					"$Banana, organic$,1,0.99\r\n",
					"Pear,7,1.24\r\n",
					"$Cake, chocolate$,1,14.50\r\n",
					"Complete the following code to ingest the file successfully.\r\n",
					"\r\n",
					"sample = spark.read.csv([...],\r\n",
					"                        sep=[...],\r\n",
					"                        header=[...],\r\n",
					"                        quote=[...],\r\n",
					"                        inferSchema=[...]\r\n",
					")\r\n",
					"(Note: If you want to test your code, sample.csv is available in the book’s repository under data/sample.csv/sample.csv). "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"\r\n",
					"df_grocery_list = spark.read.csv(\r\n",
					"    path=adls_path+\"/data/sample/sample.csv\",\r\n",
					"    sep=\",\", \r\n",
					"    header=True, \r\n",
					"    quote=\"$\", \r\n",
					"    inferSchema=True)\r\n",
					"\r\n",
					"df_grocery_list.printSchema()"
				],
				"execution_count": 29
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Add calculated columns"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logs.select(F.col(\"Duration\")).show(5)\r\n",
					"\r\n",
					"#Extract time components\r\n",
					"logs.select(\r\n",
					"    F.col(\"Duration\"),\r\n",
					"    F.col(\"Duration\").substr(1,2).cast(\"int\").alias(\"dur_hours\"),\r\n",
					"    F.col(\"Duration\").substr(4,2).cast(\"int\").alias(\"dur_minutes\"),\r\n",
					"    F.col(\"Duration\").substr(7,2).cast(\"int\").alias(\"dur_seconds\")\r\n",
					").distinct().show(5)\r\n",
					"\r\n",
					"#Calculate duration in seconds\r\n",
					"logs.select(\r\n",
					"    F.col(\"Duration\"),\r\n",
					"    (\r\n",
					"        F.col(\"Duration\").substr(1,2).cast(\"int\") * 3600 +\r\n",
					"        F.col(\"Duration\").substr(4,2).cast(\"int\") * 60 +\r\n",
					"        F.col(\"Duration\").substr(7,2).cast(\"int\")\r\n",
					"    ).alias(\"dur_seconds\")\r\n",
					").distinct().show(5)\r\n",
					"\r\n",
					"#Adding calculation as a new column into data frame\r\n",
					"logs = logs.withColumn(\r\n",
					"    \"Duration_seconds\",\r\n",
					"    F.col(\"Duration\").substr(1,2).cast(\"int\") * 3600 +\r\n",
					"    F.col(\"Duration\").substr(4,2).cast(\"int\") * 60 +\r\n",
					"    F.col(\"Duration\").substr(7,2).cast(\"int\")\r\n",
					")\r\n",
					"\r\n",
					"#Rename column\r\n",
					"logs = logs.withColumnRenamed(\"Duration_seconds\", \"dur_sec\")\r\n",
					"\r\n",
					"#Lowercasing all columns (toDF creates new dataframe from another)\r\n",
					"logs = logs.toDF(*[x.lower() for x in logs.columns]) #asterisk converts array into list of values (doesn't work without that)\r\n",
					"\r\n",
					"#Sorting columns\r\n",
					"logs = logs.select(sorted(logs.columns, reverse = True))\r\n",
					"\r\n",
					"logs.printSchema()"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Data profiling"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#for i in logs.columns:\r\n",
					"    #logs.describe(i).show(truncate = False)\r\n",
					"\r\n",
					"\r\n",
					"for i in logs.columns:\r\n",
					"    logs.select(i).summary().show() #stats can be defined as list of columns within summary() statement"
				],
				"execution_count": 31
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 4.3\r\n",
					"Reread the data in a logs_raw data frame (the data file is ./data/broadcast_logsBroadcastLogs_2018_Q3_M8.CSV), this time without passing any optional parameters. Print the first five rows of data, as well as the schema. What are the differences in terms of data and schema between logs and logs_raw?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"\r\n",
					"#Parse csv to data frame\r\n",
					"logs_raw = spark.read.csv(\r\n",
					"    path = adls_path + \"/data/broadcast_logs/BroadcastLogs_2018_Q3_M8_sample.CSV\")\r\n",
					"\r\n",
					"logs_raw.printSchema()\r\n",
					"\r\n",
					"logs_raw.show(5, 500)"
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 4.4\r\n",
					"Create a new data frame, logs_clean, that contains only the columns that do not end with ID."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"logs_clean = logs.select(*[x for x in logs.columns if x[-2:] != \"ID\"])\r\n",
					"\r\n",
					"display(logs_clean.take(10))"
				],
				"execution_count": 33
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Join operation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"log_identifier=spark.read.csv(\r\n",
					"    path=adls_path + \"/data/broadcast_logs/ReferenceTables/LogIdentifier.csv\",\r\n",
					"    sep=\"|\",\r\n",
					"    inferSchema=True,\r\n",
					"    header=True\r\n",
					")\r\n",
					"\r\n",
					"log_identifier.printSchema()\r\n",
					"\r\n",
					"log_identifier.show(5)\r\n",
					"\r\n",
					"#logs.join(\r\n",
					"#    log_identifier,\r\n",
					"#    on=(logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]) & (log_identifier[\"PrimaryFG\"] == 1)\r\n",
					"#    how=\"inner\"\r\n",
					"#).show(5)\r\n",
					"\r\n",
					"#compact version if column match and inner join (default)\r\n",
					"logs.join(\r\n",
					"    log_identifier,\r\n",
					"    on=\"LogServiceID\"\r\n",
					").show(5)\r\n",
					""
				],
				"execution_count": 43
			}
		]
	}
}