{
	"name": "DimAllTime",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ivsspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "40ec2dcd-3aaf-4e58-abea-35a84ddef90a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f939f8f3-2652-4a45-88ae-78a7782ec9df/resourceGroups/ivs-dwh-rg/providers/Microsoft.Synapse/workspaces/ivs-synapse/bigDataPools/ivsspark",
				"name": "ivsspark",
				"type": "Spark",
				"endpoint": "https://ivs-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ivsspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Read csvs and create temp views"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"fiscal_year = spark.read.csv('abfss://spark@ivsdatastorage.dfs.core.windows.net/source/FiscalYear.csv', \r\n",
					"    header = True)\r\n",
					"\r\n",
					"fiscal_year.createOrReplaceTempView('fy_view')\r\n",
					"\r\n",
					"fiscal_period = spark.read.csv('abfss://spark@ivsdatastorage.dfs.core.windows.net/source/FiscalPeriod.csv', \r\n",
					"    header = True)\r\n",
					"\r\n",
					"fiscal_period.createOrReplaceTempView('fp_view')\r\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Create merged dataframe from temporary views"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_stg = spark.sql(\"SELECT concat(CALENDAR, ',YEAR,', FISCALYEAR) DIM_ALL_TIME_ID, \" +\r\n",
					"    \"'YEAR' GRAIN_LEVEL, \"+\r\n",
					"    \"CALENDAR, \" +\r\n",
					"    \"FISCALYEAR, \" +\r\n",
					"    \"CAST(STARTDATE as timestamp) STARTDATE, \" +\r\n",
					"    \"CAST(ENDDATE as timestamp) ENDDATE, \" +\r\n",
					"    \"NULL CALENDARTYPE, \" +\r\n",
					"    \"NULL QUARTER, \" +\r\n",
					"    \"NULL MONTH, \" +\r\n",
					"    \"NULL PERIODNAME, \" +\r\n",
					"    \"NULL SHORTNAME, \" +\r\n",
					"    \"NULL TYPE, \" +\r\n",
					"    \"NULL DAYS, \" +\r\n",
					"    \"NULL PERIODNUMBER \" +\r\n",
					"    \"FROM fy_view \" +\r\n",
					"    \"UNION \" +\r\n",
					"    \"SELECT concat(CALENDAR, ',PERIOD,', FISCALYEAR, ',', replace(PERIODNAME, 'Period ', '')) DIM_ALL_TIME_ID, \" +\r\n",
					"    \"'PERIOD' GRAIN_LEVEL, \" +\r\n",
					"    \"CALENDAR, \" +\r\n",
					"    \"FISCALYEAR, \" +\r\n",
					"    \"CAST(STARTDATE as timestamp) STARTDATE, \" +\r\n",
					"    \"CAST(ENDDATE as timestamp) ENDDATE, \" +\r\n",
					"    \"CALENDARTYPE, \" +\r\n",
					"    \"QUARTER, \" +\r\n",
					"    \"MONTH, \" +\r\n",
					"    \"PERIODNAME, \" +\r\n",
					"    \"SHORTNAME, \" +\r\n",
					"    \"TYPE, \" +\r\n",
					"    \"DAYS, \" +\r\n",
					"    \"cast(replace(PERIODNAME, 'Period ', '') as int) PERIODNUMBER \" +\r\n",
					"    \"FROM fp_view \" +\r\n",
					"    \"WHERE TYPE = 1\")\r\n",
					"\r\n",
					"df_stg.show()"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Add gregorian years and months"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import lit\r\n",
					"\r\n",
					"df_stg = df_stg.select(['DIM_ALL_TIME_ID','GRAIN_LEVEL','CALENDAR','FISCALYEAR','STARTDATE','ENDDATE',\r\n",
					"    'CALENDARTYPE','QUARTER','MONTH','PERIODNAME','SHORTNAME','TYPE','DAYS','PERIODNUMBER', \r\n",
					"    lit(None).cast('integer').alias('CALYEAR'), \r\n",
					"    lit(None).cast('string').alias('CALMONTH'), \r\n",
					"    lit(None).cast('integer').alias('CALMONTH_NUM'), \r\n",
					"    lit(None).cast('integer').alias('CLOSEDMONTH')])\r\n",
					"\r\n",
					"from datetime import date\r\n",
					"from calendar import monthrange\r\n",
					"\r\n",
					"for row in df_stg.where(\"CALENDAR = 'Fiscal' and GRAIN_LEVEL = 'YEAR'\").collect():\r\n",
					"    newrow = spark.createDataFrame([('Gegorian,YEAR,' + row['FISCALYEAR'], 'YEAR', 'Gregorian', row['FISCALYEAR'], row['FISCALYEAR'] + '-01-01', row['FISCALYEAR'] + '-12-31')],\r\n",
					"        ['DIM_ALL_TIME_ID', 'GRAIN_LEVEL', 'CALENDAR', 'CALYEAR', 'STARTDATE', 'ENDDATE'])\r\n",
					"    df_stg = df_stg.unionByName(newrow, allowMissingColumns=True)\r\n",
					"    for i in range(1, 13):\r\n",
					"        newrow = spark.createDataFrame([('Gegorian,MONTH,' + row['FISCALYEAR'] + f'{i:02}', 'MONTH', 'Gregorian', \r\n",
					"            row['FISCALYEAR'], row['FISCALYEAR'] + '-' + f'{i:02}' + '-01', \r\n",
					"            row['FISCALYEAR'] + '-' + f'{i:02}' + '-' + str(monthrange(int(row['FISCALYEAR']), i)[1]),\r\n",
					"            f'{i:02}', i, \r\n",
					"            1 if row['FISCALYEAR'] + '-' + f'{i:02}' + '-' + str(monthrange(int(row['FISCALYEAR']), i)[1]) < str(date.today()) else 0)],\r\n",
					"            ['DIM_ALL_TIME_ID', 'GRAIN_LEVEL', 'CALENDAR', 'CALYEAR', 'STARTDATE', 'ENDDATE', 'CALMONTH', 'CALMONTH_NUM', 'CLOSEDMONTH'])\r\n",
					"        df_stg = df_stg.unionByName(newrow, allowMissingColumns=True)\r\n",
					"\r\n",
					"\r\n",
					"df_stg.where(\"CALENDAR = 'Gregorian' and CALYEAR = 2021\").show()"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Fiscal dates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from datetime import datetime, timedelta\r\n",
					"\r\n",
					"df_stg = df_stg.select(['DIM_ALL_TIME_ID','GRAIN_LEVEL','CALENDAR','FISCALYEAR','STARTDATE','ENDDATE',\r\n",
					"    'CALENDARTYPE','QUARTER','MONTH','PERIODNAME','SHORTNAME','TYPE','DAYS', 'PERIODNUMBER',\r\n",
					"    'CALYEAR','CALMONTH','CALMONTH_NUM','CLOSEDMONTH', \r\n",
					"    lit(None).cast('date').alias('CALENDARDATE')])\r\n",
					"\r\n",
					"for row in df_stg.where(\"CALENDAR = 'Fiscal' and GRAIN_LEVEL = 'PERIOD' and TYPE = 1\").collect():\r\n",
					"    monthRow = []\r\n",
					"    for i in range(0, (datetime.strptime(row['ENDDATE'], '%Y-%m-%d %H:%M:%S') - datetime.strptime(row['STARTDATE'], '%Y-%m-%d %H:%M:%S')).days + 1):\r\n",
					"        calDay = datetime.strptime(row['STARTDATE'], '%Y-%m-%d %H:%M:%S') + timedelta(days = i)\r\n",
					"        dayRow = ['Fiscal,DAY,' + str(calDay.year) + f'{calDay.month:02}' + f'{calDay.day:02}', 'DAY', row['CALENDAR'], row['FISCALYEAR'], row['PERIODNAME'],\r\n",
					"            calDay, calDay, calDay, row['PERIODNUMBER'], row['QUARTER'], row['MONTH'], calDay.year, f'{calDay.month:02}', calDay.month,\r\n",
					"            1 if date(calDay.year, calDay.month, monthrange(calDay.year, calDay.month)[1]) < date.today() else 0]\r\n",
					"        monthRow.append(dayRow)\r\n",
					"    newPeriod = spark.createDataFrame(monthRow, ['DIM_ALL_TIME_ID', 'GRAIN_LEVEL', 'CALENDAR', 'FISCALYEAR', 'PERIODNAME', 'CALENDARDATE', \r\n",
					"        'STARTDATE', 'ENDDATE', 'PERIODNUMBER', 'QUARTER', 'MONTH', 'CALYEAR', 'CALMONTH', 'CALMONTH_NUM', 'CLOSEDMONTH'])\r\n",
					"    df_stg = df_stg.unionByName(newPeriod, allowMissingColumns=True)\r\n",
					"\r\n",
					"df_stg.createOrReplaceTempView('AllTimeStg')\r\n",
					"\r\n",
					"df_stg.where(\"CALENDAR = 'Fiscal' and GRAIN_LEVEL = 'DAY' and FISCALYEAR = 2021 and PERIODNAME = 'Period 1'\").show()"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Weeks"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_stg = df_stg.select(['DIM_ALL_TIME_ID','GRAIN_LEVEL','CALENDAR','FISCALYEAR','STARTDATE','ENDDATE','CALENDARTYPE','QUARTER','MONTH','PERIODNAME','SHORTNAME','TYPE','DAYS', \r\n",
					"    'CALYEAR','CALMONTH','CALMONTH_NUM','CLOSEDMONTH','CALENDARDATE','PERIODNUMBER',\r\n",
					"    lit(None).cast('integer').alias('YEAR_WEEK_IN')])\r\n",
					"\r\n",
					"for row in df_stg.where(\"CALENDAR = 'Fiscal' and GRAIN_LEVEL = 'PERIOD'\").orderBy([\"FISCALYEAR\", \"PERIODNUMBER\"]).collect():\r\n",
					"    yearRow = []\r\n",
					"    weekNum = (row['PERIODNUMBER'] - 1) * 4\r\n",
					"    for i in range(0, int((datetime.strptime(row['ENDDATE'], '%Y-%m-%d %H:%M:%S') - datetime.strptime(row['STARTDATE'], '%Y-%m-%d %H:%M:%S')).days/7)):\r\n",
					"        weekStart = datetime.strptime(row['STARTDATE'], '%Y-%m-%d %H:%M:%S') + timedelta(days = 7*i)\r\n",
					"        weekNum = weekNum + 1\r\n",
					"        weekRow = ['Fiscal,WEEK,' + str(row['FISCALYEAR']) + ',' + str(weekNum), \r\n",
					"            'WEEK', row['CALENDAR'], row['FISCALYEAR'], row['PERIODNAME'],\r\n",
					"            weekStart, weekStart + timedelta(days = 6), row['PERIODNUMBER'],\r\n",
					"            row['QUARTER'], row['MONTH'], weekNum]\r\n",
					"        yearRow.append(weekRow)\r\n",
					"    newYear = spark.createDataFrame(yearRow, ['DIM_ALL_TIME_ID', 'GRAIN_LEVEL', 'CALENDAR', 'FISCALYEAR', 'PERIODNAME', \r\n",
					"        'STARTDATE', 'ENDDATE', 'PERIODNUMBER', 'QUARTER', 'MONTH', 'YEAR_WEEK_IN'])\r\n",
					"    df_stg = df_stg.unionByName(newYear, allowMissingColumns=True)\r\n",
					"\r\n",
					"df_stg.createOrReplaceTempView('AllTimeStg')\r\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"select * from AllTimeStg\r\n",
					"where fiscalyear = 2019\r\n",
					"order by startdate\r\n",
					"limit 20\r\n",
					"\r\n",
					""
				],
				"execution_count": 27
			}
		]
	}
}