{
	"name": "DimAllTime",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ivsspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "eb986576-31db-4f1b-a8e0-e74529f63b39"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f939f8f3-2652-4a45-88ae-78a7782ec9df/resourceGroups/ivs-dwh-rg/providers/Microsoft.Synapse/workspaces/ivs-synapse/bigDataPools/ivsspark",
				"name": "ivsspark",
				"type": "Spark",
				"endpoint": "https://ivs-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ivsspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Read csvs and create temp views"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"fiscal_year = spark.read.csv('abfss://spark@ivsdatastorage.dfs.core.windows.net/source/FiscalYear.csv', \r\n",
					"    header = True)\r\n",
					"\r\n",
					"fiscal_year.createOrReplaceTempView('fy_view')\r\n",
					"\r\n",
					"fiscal_period = spark.read.csv('abfss://spark@ivsdatastorage.dfs.core.windows.net/source/FiscalPeriod.csv', \r\n",
					"    header = True)\r\n",
					"\r\n",
					"fiscal_period.createOrReplaceTempView('fp_view')\r\n",
					""
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_stg = spark.sql(\"SELECT concat(CALENDAR, ',YEAR,', FISCALYEAR) DIM_ALL_TIME_ID, \" +\r\n",
					"    \"'YEAR' GRAIN_LEVEL, \"+\r\n",
					"    \"CALENDAR, \" +\r\n",
					"    \"FISCALYEAR, \" +\r\n",
					"    \"CAST(STARTDATE as timestamp) STARTDATE, \" +\r\n",
					"    \"CAST(ENDDATE as timestamp) ENDDATE, \" +\r\n",
					"    \"NULL CALENDARTYPE, \" +\r\n",
					"    \"NULL QUARTER, \" +\r\n",
					"    \"NULL MONTH, \" +\r\n",
					"    \"NULL PERIODNAME, \" +\r\n",
					"    \"NULL SHORTNAME, \" +\r\n",
					"    \"NULL TYPE, \" +\r\n",
					"    \"NULL DAYS \" +\r\n",
					"    \"FROM fy_view \" +\r\n",
					"    \"UNION \" +\r\n",
					"    \"SELECT concat(CALENDAR, ',PERIOD,', FISCALYEAR, ',', replace(PERIODNAME, 'Period ', '')) DIM_ALL_TIME_ID, \" +\r\n",
					"    \"'PERIOD' GRAIN_LEVEL, \" +\r\n",
					"    \"CALENDAR, \" +\r\n",
					"    \"FISCALYEAR, \" +\r\n",
					"    \"CAST(STARTDATE as timestamp) STARTDATE, \" +\r\n",
					"    \"CAST(ENDDATE as timestamp) ENDDATE, \" +\r\n",
					"    \"CALENDARTYPE, \" +\r\n",
					"    \"QUARTER, \" +\r\n",
					"    \"MONTH, \" +\r\n",
					"    \"PERIODNAME, \" +\r\n",
					"    \"SHORTNAME, \" +\r\n",
					"    \"TYPE, \" +\r\n",
					"    \"DAYS \" +\r\n",
					"    \"FROM fp_view \")\r\n",
					"\r\n",
					"df_stg.show()"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import lit\r\n",
					"\r\n",
					"df_stg = df_stg.withColumn('CALYEAR', lit(None).cast('integer'))\r\n",
					"\r\n",
					"for row in df_stg.where(\"CALENDAR = 'Fiscal' and GRAIN_LEVEL = 'YEAR'\").collect():\r\n",
					"    newrow = spark.createDataFrame([('Gegorian,YEAR,' + row['FISCALYEAR'], row['GRAIN_LEVEL'], 'Gregorian', row['FISCALYEAR'], row['FISCALYEAR'] + '-01-01', row['FISCALYEAR'] + '-12-31')],\r\n",
					"        ['DIM_ALL_TIME_ID', 'GRAIN_LEVEL', 'CALENDAR', 'CALYEAR', 'STARTDATE', 'ENDDATE'])\r\n",
					"    df_stg = df_stg.unionByName(newrow, allowMissingColumns=True)\r\n",
					"\r\n",
					"df_stg.where(\"GRAIN_LEVEL = 'YEAR'\").show()\r\n",
					""
				],
				"execution_count": 57
			}
		]
	}
}