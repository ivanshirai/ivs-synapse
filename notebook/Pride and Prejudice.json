{
	"name": "Pride and Prejudice",
	"properties": {
		"folder": {
			"name": "Data Analysis with Python and PySpark"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ivsspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6116c707-7f3e-41bf-a2c9-0847c9166a60"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f939f8f3-2652-4a45-88ae-78a7782ec9df/resourceGroups/ivs-dwh-rg/providers/Microsoft.Synapse/workspaces/ivs-synapse/bigDataPools/ivsspark",
				"name": "ivsspark",
				"type": "Spark",
				"endpoint": "https://ivs-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ivsspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Activate eager evaluation (instead of lazy)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#from pyspark.sql import SparkSession\r\n",
					" \r\n",
					"#spark = (SparkSession.builder\r\n",
					"#                     .config(\"spark.sql.repl.eagerEval.enabled\", \"True\")\r\n",
					"#                     .getOrCreate())"
				],
				"execution_count": 138
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Reading data into a data frame with spark.read"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"\r\n",
					"#Load text file to data frame\r\n",
					"book = spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"\r\n",
					"book\r\n",
					"\r\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"book.printSchema()\r\n",
					"\r\n",
					"print(book.dtypes)\r\n",
					"\r\n",
					"#print(spark.__doc__)\r\n",
					"\r\n",
					"dir(book)"
				],
				"execution_count": 140
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Code assistance"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#General recommendations\r\n",
					"print(spark.__doc__)\r\n",
					"\r\n",
					"#Methods available for the object\r\n",
					"dir(book)\r\n",
					"\r\n",
					"#Documentation on the object\r\n",
					"book.__doc__"
				],
				"execution_count": 141
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#book.show()\r\n",
					"\r\n",
					"book.show(n=10, truncate=100, vertical=True)"
				],
				"execution_count": 142
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Splitting lines of text into arrays or words"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import split\r\n",
					"\r\n",
					"lines = book.select(split(book.value, ' ').alias('line'))\r\n",
					"\r\n",
					"#Alternative ways to select data:\r\n",
					"#book.select(book.value)\r\n",
					"#book.select(book[\"value\"])\r\n",
					"#book.select(col(\"value\"))\r\n",
					"#book.select(\"value\")\r\n",
					"\r\n",
					"#Alternative ways to alias data:\r\n",
					"# This looks a lot cleaner\r\n",
					"#lines = book.select(split(book.value, \" \").alias(\"line\"))\r\n",
					"# This is messier, and you have to remember the name PySpark assigns automatically\r\n",
					"#lines = book.select(split(book.value, \" \"))\r\n",
					"#lines = lines.withColumnRenamed(\"split(value, , -1)\", \"line\")\r\n",
					"\r\n",
					"lines.show(5, 100)\r\n",
					"\r\n",
					"lines.printSchema()\r\n",
					"\r\n",
					""
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exploding arrays into rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import explode\r\n",
					"\r\n",
					"words = lines.select(explode(lines.line).alias(\"word\"))\r\n",
					"\r\n",
					"words.show(30, 30)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Cleansing data (lowering case, remove punctuation and empty words)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import lower, regexp_extract\r\n",
					"\r\n",
					"words_lower = words.select(lower(words.word).alias('word_lower'))\r\n",
					"\r\n",
					"words_clean = words_lower.select(regexp_extract(words_lower.word_lower, \"[a-z]+\", 0).alias(\"word\"))\r\n",
					"\r\n",
					"words_not_null = words_clean.filter(words_clean.word != \"\")\r\n",
					"\r\n",
					"words_not_null.show(30,30)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.2\r\n",
					"Given the following data frame, programmatically count the number of columns that aren’t strings (answer = only one column isn’t a string).\r\n",
					"\r\n",
					"createDataFrame() allows you to create a data frame from a variety of sources, such as a pandas data frame or (in this case) a list of lists."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"exo2_2_df = spark.createDataFrame(\r\n",
					"    [[\"test\", \"more test\", 10_000_000_000]], [\"one\", \"two\", \"three\"]\r\n",
					")\r\n",
					" \r\n",
					"#exo2_2_df.printSchema()\r\n",
					"\r\n",
					"dfcolumns = spark.createDataFrame(exo2_2_df.dtypes, ['name', 'type'])\r\n",
					"\r\n",
					"dfcolumns_filtered = dfcolumns.filter(dfcolumns.type != 'string')\r\n",
					"\r\n",
					"print('Non-string columns number:')\r\n",
					"print(dfcolumns_filtered.count())\r\n",
					""
				],
				"execution_count": 160
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.3\r\n",
					"Rewrite the following code snippet, removing the withColumnRenamed method. Which version is clearer and easier to read?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"\r\n",
					"from pyspark.sql.functions import col, length\r\n",
					" \r\n",
					"# The `length` function returns the number of characters in a string column.\r\n",
					" \r\n",
					"exo2_3_df = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(length(col(\"value\")))\r\n",
					"    .withColumnRenamed(\"length(value)\", \"number_of_char\")\r\n",
					")\r\n",
					"\r\n",
					"exo2_3_df.show() .withColumnRenamed easier to read\r\n",
					"\r\n",
					"#Version with "
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.4\r\n",
					"Assume a data frame exo2_4_df. The following code block gives an error. What is the problem, and how can you solve it?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import col, greatest\r\n",
					" \r\n",
					"exo2_4_df = spark.createDataFrame(\r\n",
					"    [[\"key\", 10_000, 20_000]], [\"key\", \"value1\", \"value2\"]\r\n",
					")\r\n",
					" \r\n",
					"exo2_4_df.printSchema()\r\n",
					"# root\r\n",
					"#  |-- key: string (containsNull = true)\r\n",
					"#  |-- value1: long (containsNull = true)\r\n",
					"#  |-- value2: long (containsNull = true)\r\n",
					" \r\n",
					"# `greatest` will return the greatest value of the list of column names,\r\n",
					"# skipping null value\r\n",
					" \r\n",
					"# The following statement will return an error\r\n",
					"from pyspark.sql.utils import AnalysisException\r\n",
					" \r\n",
					"try:\r\n",
					"    exo2_4_mod = exo2_4_df.select(\r\n",
					"        greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\r\n",
					"    ).select(\"key\", \"max_value\")\r\n",
					"except AnalysisException as err:\r\n",
					"    print(err)\r\n",
					"\r\n",
					"#modified code\r\n",
					"exo2_4_df.select(col(\"key\"),\r\n",
					"        greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\r\n",
					"    ).select(\"key\", \"maximum_value\").show()"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.5\r\n",
					"Let’s take our words_nonull data frame, available in the next listing. You can use the code from the repository (code/Ch02/end_of_chapter.py) in your REPL to get the data frame loaded.\r\n",
					"\r\n",
					"a) Remove all of the occurrences of the word is.\r\n",
					"\r\n",
					"b) (Challenge) Using the length function, keep only the words with more than three characters."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#words_not_null.show()\r\n",
					"\r\n",
					"#a\r\n",
					"words_no_is = words_not_null.filter(col(\"word\") != 'is')\r\n",
					"\r\n",
					"#words_no_is.show()\r\n",
					"\r\n",
					"#b\r\n",
					"words_longer_than_3 = (words_not_null.select(col(\"word\"), length(col(\"word\")).alias(\"word_length\"))\r\n",
					"    .filter(col(\"word_length\") > 3)\r\n",
					"    .select(\"word\")\r\n",
					")\r\n",
					"\r\n",
					"#words_longer_than_3.show()"
				],
				"execution_count": 28
			}
		]
	}
}