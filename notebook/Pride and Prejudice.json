{
	"name": "Pride and Prejudice",
	"properties": {
		"folder": {
			"name": "Data Analysis with Python and PySpark"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ivsspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4b197407-b31b-43f4-9d20-566850138adb"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f939f8f3-2652-4a45-88ae-78a7782ec9df/resourceGroups/ivs-dwh-rg/providers/Microsoft.Synapse/workspaces/ivs-synapse/bigDataPools/ivsspark",
				"name": "ivsspark",
				"type": "Spark",
				"endpoint": "https://ivs-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ivsspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Activate eager evaluation (instead of lazy)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#from pyspark.sql import SparkSession\r\n",
					" \r\n",
					"#spark = (SparkSession.builder\r\n",
					"#                     .config(\"spark.sql.repl.eagerEval.enabled\", \"True\")\r\n",
					"#                     .getOrCreate())"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Reading data into a data frame with spark.read"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"\r\n",
					"#Load text file to data frame\r\n",
					"book = spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"\r\n",
					"book\r\n",
					"\r\n",
					""
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"book.printSchema()\r\n",
					"\r\n",
					"print(book.dtypes)\r\n",
					"\r\n",
					"#print(spark.__doc__)\r\n",
					"\r\n",
					"dir(book)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Code assistance"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#General recommendations\r\n",
					"print(spark.__doc__)\r\n",
					"\r\n",
					"#Methods available for the object\r\n",
					"dir(book)\r\n",
					"\r\n",
					"#Documentation on the object\r\n",
					"book.__doc__"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#book.show()\r\n",
					"\r\n",
					"book.show(n=10, truncate=100, vertical=True)"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Splitting lines of text into arrays or words"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import split\r\n",
					"\r\n",
					"lines = book.select(split(book.value, ' ').alias('line'))\r\n",
					"\r\n",
					"#Alternative ways to select data:\r\n",
					"#book.select(book.value)\r\n",
					"#book.select(book[\"value\"])\r\n",
					"#book.select(col(\"value\"))\r\n",
					"#book.select(\"value\")\r\n",
					"\r\n",
					"#Alternative ways to alias data:\r\n",
					"# This looks a lot cleaner\r\n",
					"#lines = book.select(split(book.value, \" \").alias(\"line\"))\r\n",
					"# This is messier, and you have to remember the name PySpark assigns automatically\r\n",
					"#lines = book.select(split(book.value, \" \"))\r\n",
					"#lines = lines.withColumnRenamed(\"split(value, , -1)\", \"line\")\r\n",
					"\r\n",
					"lines.show(5, 100)\r\n",
					"\r\n",
					"lines.printSchema()\r\n",
					"\r\n",
					""
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exploding arrays into rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import explode\r\n",
					"\r\n",
					"words = lines.select(explode(lines.line).alias(\"word\"))\r\n",
					"\r\n",
					"words.show(30, 30)"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Cleansing data (lowering case, remove punctuation and empty words)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import lower, regexp_extract\r\n",
					"\r\n",
					"words_lower = words.select(lower(words.word).alias('word_lower'))\r\n",
					"\r\n",
					"words_clean = words_lower.select(regexp_extract(words_lower.word_lower, \"[a-z]+\", 0).alias(\"word\"))\r\n",
					"\r\n",
					"words_not_null = words_clean.filter(words_clean.word != \"\")\r\n",
					"\r\n",
					"words_not_null.show(30,30)"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.2\r\n",
					"Given the following data frame, programmatically count the number of columns that aren’t strings (answer = only one column isn’t a string).\r\n",
					"\r\n",
					"createDataFrame() allows you to create a data frame from a variety of sources, such as a pandas data frame or (in this case) a list of lists."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"exo2_2_df = spark.createDataFrame(\r\n",
					"    [[\"test\", \"more test\", 10_000_000_000]], [\"one\", \"two\", \"three\"]\r\n",
					")\r\n",
					" \r\n",
					"#exo2_2_df.printSchema()\r\n",
					"\r\n",
					"dfcolumns = spark.createDataFrame(exo2_2_df.dtypes, ['name', 'type'])\r\n",
					"\r\n",
					"dfcolumns_filtered = dfcolumns.filter(dfcolumns.type != 'string')\r\n",
					"\r\n",
					"print('Non-string columns number:')\r\n",
					"print(dfcolumns_filtered.count())\r\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.3\r\n",
					"Rewrite the following code snippet, removing the withColumnRenamed method. Which version is clearer and easier to read?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"\r\n",
					"from pyspark.sql.functions import col, length\r\n",
					" \r\n",
					"# The `length` function returns the number of characters in a string column.\r\n",
					" \r\n",
					"exo2_3_df = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(length(col(\"value\")))\r\n",
					"    .withColumnRenamed(\"length(value)\", \"number_of_char\")\r\n",
					")\r\n",
					"\r\n",
					"exo2_3_df.show() .withColumnRenamed easier to read\r\n",
					"\r\n",
					"#Version with "
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.4\r\n",
					"Assume a data frame exo2_4_df. The following code block gives an error. What is the problem, and how can you solve it?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import col, greatest\r\n",
					" \r\n",
					"exo2_4_df = spark.createDataFrame(\r\n",
					"    [[\"key\", 10_000, 20_000]], [\"key\", \"value1\", \"value2\"]\r\n",
					")\r\n",
					" \r\n",
					"exo2_4_df.printSchema()\r\n",
					"# root\r\n",
					"#  |-- key: string (containsNull = true)\r\n",
					"#  |-- value1: long (containsNull = true)\r\n",
					"#  |-- value2: long (containsNull = true)\r\n",
					" \r\n",
					"# `greatest` will return the greatest value of the list of column names,\r\n",
					"# skipping null value\r\n",
					" \r\n",
					"# The following statement will return an error\r\n",
					"from pyspark.sql.utils import AnalysisException\r\n",
					" \r\n",
					"try:\r\n",
					"    exo2_4_mod = exo2_4_df.select(\r\n",
					"        greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\r\n",
					"    ).select(\"key\", \"max_value\")\r\n",
					"except AnalysisException as err:\r\n",
					"    print(err)\r\n",
					"\r\n",
					"#modified code\r\n",
					"exo2_4_df.select(col(\"key\"),\r\n",
					"        greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\r\n",
					"    ).select(\"key\", \"maximum_value\").show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.5\r\n",
					"Let’s take our words_nonull data frame, available in the next listing. You can use the code from the repository (code/Ch02/end_of_chapter.py) in your REPL to get the data frame loaded.\r\n",
					"\r\n",
					"a) Remove all of the occurrences of the word is.\r\n",
					"\r\n",
					"b) (Challenge) Using the length function, keep only the words with more than three characters."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#words_not_null.show()\r\n",
					"\r\n",
					"#a\r\n",
					"words_no_is = words_not_null.filter(col(\"word\") != 'is')\r\n",
					"\r\n",
					"#words_no_is.show()\r\n",
					"\r\n",
					"#b\r\n",
					"words_longer_than_3 = (words_not_null.select(col(\"word\"), length(col(\"word\")).alias(\"word_length\"))\r\n",
					"    .filter(col(\"word_length\") > 3)\r\n",
					"    .select(\"word\")\r\n",
					")\r\n",
					"\r\n",
					"#words_longer_than_3.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The where clause takes a Boolean expression over one or many columns to filter the data frame. Beyond the usual Boolean operators (>, <, ==, <=, >=, !=), PySpark provides other functions that return Boolean columns in the pyspark.sql.functions module.\r\n",
					"\r\n",
					"A good example is the isin() method (applied on a Column object, like col(...).isin(...)), which takes a list of values as a parameter, and will return only the records where the value in the column equals a member of the list.\r\n",
					"\r\n",
					"Let’s say you want to remove the words is, not, the and if from your list of words, using a single where() method on the words_nonull data frame. Write the code to do so."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"no_short_words = words_not_null.where(~col(\"word\").isin(\"is\", \"not\", \"the\"))\r\n",
					"\r\n",
					"#no_short_words = words_not_null.where(col(\"word\").isin(\"is\", \"not\", \"the\") == False)\r\n",
					"\r\n",
					"no_short_words.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 2.7\r\n",
					"One of your friends comes to you with the following code. They have no idea why it doesn’t work. Can you diagnose the problem in the try block, explain why it is an error, and provide a fix?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import col, split\r\n",
					"\r\n",
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					" \r\n",
					"try:\r\n",
					"    book = spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    book = book.printSchema()\r\n",
					"    lines = book.select(split(book.value, \" \").alias(\"line\"))\r\n",
					"    words = lines.select(explode(col(\"line\")).alias(\"word\"))\r\n",
					"except AnalysisException as err:\r\n",
					"    print(err)\r\n",
					"\r\n",
					"\r\n",
					"#fix:\r\n",
					"try:\r\n",
					"    book = spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"#    book = book.printSchema()\r\n",
					"    lines = book.select(split(book.value, \" \").alias(\"line\"))\r\n",
					"    words = lines.select(explode(col(\"line\")).alias(\"word\"))\r\n",
					"except AnalysisException as err:\r\n",
					"    print(err)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Book wording analysis continuation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"groups = words_not_null.groupby(col(\"word\"))\r\n",
					"\r\n",
					"print(groups)\r\n",
					"\r\n",
					"result = words_not_null.groupby(col(\"word\")).count()\r\n",
					"\r\n",
					"result.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 3.1\r\n",
					"\r\n",
					"Starting with the words_not_null seen in this section, which of the following expressions would return the number of words per letter count (e.g., there are X one-letter words, Y two-letter words, etc.)?\r\n",
					"\r\n",
					"Assume that pyspark.sql.functions.col, pyspark.sql.functions.length are imported.\r\n",
					"\r\n",
					"a) words_not_null.select(length(col(\"word\"))).groupby(\"length\").count()\r\n",
					"\r\n",
					"b) words_not_null.select(length(col(\"word\")).alias(\"length\")).groupby(\"length\").count()\r\n",
					"\r\n",
					"c) words_not_null.groupby(\"length\").select(\"length\").count()\r\n",
					"\r\n",
					"d) None of those options would work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"words_not_null.select(length(col(\"word\")).alias(\"length\")).groupby(\"length\").count().show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Book wording analysis continuation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"result = words_not_null.groupby(col(\"word\")).count().orderBy(col(\"count\"), ascending = False)\r\n",
					"\r\n",
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"\r\n",
					"result.write.csv(adls_path + \"/output/Pride and Prejudice/partitioned\")\r\n",
					"\r\n",
					"result.coalesce(1).write.csv(adls_path + \"/output/Pride and Prejudice/single file\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Simplifying code"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"\r\n",
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					" \r\n",
					"results = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"    .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"    .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"    .where(F.col(\"word\") != \"\")\r\n",
					"    .groupby(\"word\")\r\n",
					"    .count()\r\n",
					"    .orderBy(F.col(\"count\"), ascending = False)\r\n",
					")\r\n",
					"\r\n",
					"results.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Processing more books"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"\r\n",
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					" \r\n",
					"results = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/*.txt\")\r\n",
					"    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"    .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"    .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"    .where(F.col(\"word\") != \"\")\r\n",
					"    .groupby(\"word\")\r\n",
					"    .count()\r\n",
					"    .orderBy(F.col(\"count\"), ascending = False)\r\n",
					")\r\n",
					"\r\n",
					"results.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 3.3\r\n",
					"By modifying the word_count_submit.py program, return the number of distinct words in Jane Austen’s Pride and Prejudice. (Hint: results contains one record for each unique word.)\r\n",
					"\r\n",
					"(Challenge) Wrap your program in a function that takes a file name as a parameter. It should return the number of distinct words."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"\r\n",
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					" \r\n",
					"result = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"    .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"    .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"    .where(F.col(\"word\") != \"\")\r\n",
					"    .select(F.col(\"word\"))\r\n",
					"    .distinct()\r\n",
					"    .count()\r\n",
					")\r\n",
					"\r\n",
					"print(result)\r\n",
					"\r\n",
					"def bookDistCount(fName):\r\n",
					"    #Root path var\r\n",
					"    adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					"    \r\n",
					"    result = (\r\n",
					"        spark.read.text(adls_path + \"/data/gutenberg_books/\" + fName)\r\n",
					"        .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"        .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"        .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"        .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"        .where(F.col(\"word\") != \"\")\r\n",
					"        .select(F.col(\"word\"))\r\n",
					"        .distinct()\r\n",
					"        .count()\r\n",
					"    )\r\n",
					"\r\n",
					"    return result\r\n",
					"\r\n",
					"\r\n",
					"print(bookDistCount(\"1661-0.txt\"))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 3.4\r\n",
					"Taking word_count_submit.py, modify the script to return a sample of five words that appear only once in Jane Austen’s Pride and Prejudice."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Root path var\r\n",
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					" \r\n",
					"results = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"    .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"    .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"    .where(F.col(\"word\") != \"\")\r\n",
					"    .groupby(\"word\")\r\n",
					"    .count()\r\n",
					"    .where(F.col(\"count\") == 1)\r\n",
					")\r\n",
					"\r\n",
					"results.show(5)"
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 3.5\r\n",
					"Using the substring function (refer to PySpark’s API or the pyspark shell if needed), return the top five most popular first letters (keep only the first letter of each word).\r\n",
					"\r\n",
					"Compute the number of words starting with a consonant or a vowel. (Hint: The isin() function might be useful.)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					" \r\n",
					"results = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"    .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"    .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"    .where(F.col(\"word\") != \"\")\r\n",
					"    .groupby(F.substring(F.col(\"word\"), 1, 1).alias(\"firstLetter\"))\r\n",
					"    .count()\r\n",
					"    .orderBy(F.col(\"count\"), ascending = False)\r\n",
					")\r\n",
					"\r\n",
					"results.show(5)\r\n",
					"\r\n",
					"results_vowels = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"    .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"    .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"    .where(F.col(\"word\") != \"\")\r\n",
					"    .groupby(F.substring(F.col(\"word\"), 1, 1).alias(\"firstLetter\"))\r\n",
					"    .count()\r\n",
					"    .where(F.col(\"firstLetter\").isin(['a','e','i','o','u','y']))\r\n",
					"    .orderBy(F.col(\"count\"), ascending = False)\r\n",
					")\r\n",
					"\r\n",
					"results_vowels.show(5)\r\n",
					"\r\n",
					"results_consonants = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"    .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"    .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"    .where(F.col(\"word\") != \"\")\r\n",
					"    .groupby(F.substring(F.col(\"word\"), 1, 1).alias(\"firstLetter\"))\r\n",
					"    .count()\r\n",
					"    .where(~F.col(\"firstLetter\").isin(['a','e','i','o','u','y']))\r\n",
					"    .orderBy(F.col(\"count\"), ascending = False)\r\n",
					")\r\n",
					"\r\n",
					"results_consonants.show(5)"
				],
				"execution_count": 37
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Exercise 3.6\r\n",
					"Let’s say you want to get both the count() and sum() of a GroupedData object. Why doesn’t this code work? Map the inputs and outputs of each method.\r\n",
					"\r\n",
					"my_data_frame.groupby(\"my_column\").count().sum()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"adls_path = \"abfss://learning@ivsdatastorage.dfs.core.windows.net/DataAnalysisPythonPySpark\"\r\n",
					" \r\n",
					"results = (\r\n",
					"    spark.read.text(adls_path + \"/data/gutenberg_books/1342-0.txt\")\r\n",
					"    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\r\n",
					"    .select(F.explode(F.col(\"line\")).alias(\"word\"))\r\n",
					"    .select(F.lower(F.col(\"word\")).alias(\"word\"))\r\n",
					"    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\r\n",
					"    .where(F.col(\"word\") != \"\")\r\n",
					"    .groupby(F.col(\"word\"))\r\n",
					"    .count()\r\n",
					"    .groupby(F.col(\"word\"))\r\n",
					"    .sum(\"count\")\r\n",
					")\r\n",
					"\r\n",
					"results.show()"
				],
				"execution_count": 49
			}
		]
	}
}